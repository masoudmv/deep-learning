{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# HW3 - RNN\n",
   "metadata": {
    "id": "O5ZbzcSP60pK"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Homework Overview\nIn this homework, you will learn to implement, train, and evaluate Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) models on a text classification task using a dataset of IMDB movie reviews, and compare them.\n\n**NOTE : Be sure to answer the analytical questions at the end of the notebook as well.**",
   "metadata": {
    "id": "pI5BRkcqEvSe"
   }
  },
  {
   "cell_type": "code",
   "source": "# Imports\nimport nltk\nnltk.download('stopwords')\nimport os\nimport random\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom nltk import wordpunct_tokenize\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, classification_report\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom IPython.core.display import display, HTML\nfrom datasets import load_dataset\n\n# Enable tqdm progress bar in pandas\ntqdm.pandas()\n\n# Set device (GPU or CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zqJpTGRcqgrB",
    "outputId": "72367c89-2a8f-4a6a-c629-3a59f4e95d78",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:07:45.585456Z",
     "iopub.execute_input": "2025-05-31T16:07:45.585678Z",
     "iopub.status.idle": "2025-05-31T16:07:52.963242Z",
     "shell.execute_reply.started": "2025-05-31T16:07:45.585653Z",
     "shell.execute_reply": "2025-05-31T16:07:52.962600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Using device: cuda\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": "# Dataset\n\nIn this section, we’ll load the IMDB dataset and preprocess the data to make it suitable for training RNN and LSTM models.",
   "metadata": {
    "id": "1NKWaZBn8NMY"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Load Dataset\nDescription of Dataset: The IMDB movie reviews dataset consists of reviews along with their labels (positive or negative sentiment). Each review is a sentence or paragraph of text.\n\nDownload the Dataset: We will use a Huggingface to download the dataset into our environment.",
   "metadata": {
    "id": "D_9UAw1I8uvP"
   }
  },
  {
   "cell_type": "code",
   "source": "# Load the IMDb dataset (Hugging Face)\ndataset = load_dataset(\"imdb\")\n\n# Make sure 'data' folder exists\nos.makedirs('data', exist_ok=True)\n\n# Combine train and test splits into one DataFrame\ntrain_df = pd.DataFrame(dataset['train'])\ntest_df = pd.DataFrame(dataset['test'])\nfull_df = pd.concat([train_df, test_df], ignore_index=True)\n\n# Save combined DataFrame to CSV\nDATA_PATH = 'data/imdb_reviews.csv'\nfull_df.to_csv(DATA_PATH, index=False)\n\nprint(f\"✅ Saved combined IMDb reviews to: {DATA_PATH}\")\nprint(f\"Total samples: {len(full_df)}\")",
   "metadata": {
    "id": "s463eDIjmGmc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "76c76f47-a3f4-4b16-9a93-b261d58d6e52",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:08:02.609865Z",
     "iopub.execute_input": "2025-05-31T16:08:02.610151Z",
     "iopub.status.idle": "2025-05-31T16:08:10.823550Z",
     "shell.execute_reply.started": "2025-05-31T16:08:02.610130Z",
     "shell.execute_reply": "2025-05-31T16:08:10.822738Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64b6e72b99114f71b13d36fd89d3ff30"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fcb542e35f314647b6a9ac94ff79d33e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e4f005a976654eef9436f1140f600c7c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92dff1cc0e964b5b9b840c706a89efe8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee15116393bb4914bdc623e6f41c151d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a7ec216c73642fa9d3a55f5b149a174"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79d3f012206241b49f9393a6c83b198b"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "✅ Saved combined IMDb reviews to: data/imdb_reviews.csv\nTotal samples: 50000\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "# Show 5 random samples from the DataFrame\nprint(full_df.sample(5))",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RKHX3Zh6ubsr",
    "outputId": "01472428-67f2-4ceb-85a0-ae56d12e1c90",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:08:10.824611Z",
     "iopub.execute_input": "2025-05-31T16:08:10.824829Z",
     "iopub.status.idle": "2025-05-31T16:08:10.840782Z",
     "shell.execute_reply.started": "2025-05-31T16:08:10.824813Z",
     "shell.execute_reply": "2025-05-31T16:08:10.840175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "                                                    text  label\n48263  I found it charming! Nobody else but Kiarostam...      1\n28205  I'm usually not inclined to write reviews abou...      0\n25235  i came across this film on the net by fluke an...      0\n8808   Let me start out by saying I can enjoy just ab...      0\n232    This is really terrible.<br /><br />The only r...      0\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": "## Preprocessing\n\nFor our models to work effectively, we need to preprocess the text data by cleaning it and converting words to integer indices for training.Preproces steps\nsuch as Tokenization and Cleaning , Replacing Rare Words , Build Vocabulary , Convert Tokens to Indices and Prepare Data for Training.\n\n**NOTE : Do not alter the structure of this preprocessing code, as it aligns with other parts of the notebook.However, minor adjustments for compatibility with your code are allowed if needed.**",
   "metadata": {
    "id": "apPnMJrH9AG4"
   }
  },
  {
   "cell_type": "code",
   "source": "def tokenize(text, stop_words):\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = text.lower()\n    tokens = wordpunct_tokenize(text)\n    tokens = [token for token in tokens if token not in stop_words]\n    return tokens",
   "metadata": {
    "id": "Y8egndGf-GIR",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:08:13.511828Z",
     "iopub.execute_input": "2025-05-31T16:08:13.512113Z",
     "iopub.status.idle": "2025-05-31T16:08:13.517147Z",
     "shell.execute_reply.started": "2025-05-31T16:08:13.512091Z",
     "shell.execute_reply": "2025-05-31T16:08:13.516614Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "def remove_rare_words(tokens, common_tokens, max_len):\n    return [token if token in common_tokens\n            else '<UNK>' for token in tokens][-max_len:]",
   "metadata": {
    "id": "XNlaFTd-mJuS",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:08:13.810638Z",
     "iopub.execute_input": "2025-05-31T16:08:13.811098Z",
     "iopub.status.idle": "2025-05-31T16:08:13.814634Z",
     "shell.execute_reply.started": "2025-05-31T16:08:13.811080Z",
     "shell.execute_reply": "2025-05-31T16:08:13.813985Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "def load_and_preprocess_data(data_path, max_vocab, max_len):\n    df = pd.read_csv(data_path)\n    stop_words = set(stopwords.words('english'))\n\n    # Clean and tokenize\n    df['tokens'] = df['text'].apply(lambda x: tokenize(x, stop_words))\n\n    # Replace rare words with <UNK>\n    all_tokens = [token for tokens in df['tokens'] for token in tokens]\n    common_tokens = set(list(zip(*Counter(all_tokens).most_common(max_vocab)))[0])\n    df['tokens'] = df['tokens'].apply(lambda x: remove_rare_words(x, common_tokens, max_len))\n\n    # Remove sequences with only <UNK>\n    df = df[df['tokens'].apply(lambda tokens: any(token != '<UNK>' for token in tokens))]\n\n    # Build vocab\n    vocab = sorted(set([token for tokens in df['tokens'] for token in tokens]))\n    token2idx = {token: idx for idx, token in enumerate(vocab)}\n    token2idx['<PAD>'] = len(token2idx)\n\n    # Index tokens\n    df['indexed_tokens'] = df['tokens'].apply(lambda tokens: [token2idx[token] for token in tokens])\n\n    return df['indexed_tokens'].tolist(), df['label'].tolist(), token2idx",
   "metadata": {
    "id": "zsJ38mMjmObb",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:08:15.434785Z",
     "iopub.execute_input": "2025-05-31T16:08:15.435066Z",
     "iopub.status.idle": "2025-05-31T16:08:15.441391Z",
     "shell.execute_reply.started": "2025-05-31T16:08:15.435045Z",
     "shell.execute_reply": "2025-05-31T16:08:15.440576Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "# How many of the most common vocab words to keep\n# Uncommon words get replaced with unknown token <UNK>\nmax_vocab = 2500\n\n# How many tokens long each sequence will be cut to\n# Shorter sequences will get the padding token <PAD>\nmax_len = 100\n\nsequences, targets, token2idx = load_and_preprocess_data(DATA_PATH, max_vocab, max_len)\n",
   "metadata": {
    "id": "S86p_iDJmRbs",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:08:34.748968Z",
     "iopub.execute_input": "2025-05-31T16:08:34.749670Z",
     "iopub.status.idle": "2025-05-31T16:08:44.375242Z",
     "shell.execute_reply.started": "2025-05-31T16:08:34.749646Z",
     "shell.execute_reply": "2025-05-31T16:08:44.374666Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "def split_data(sequences, targets, valid_ratio=0.05, test_ratio=0.05):\n    total_size = len(sequences)\n    test_size = int(total_size * test_ratio)\n    valid_size = int(total_size * valid_ratio)\n    train_size = total_size - valid_size - test_size\n\n    train_sequences, train_targets = sequences[:train_size], targets[:train_size]\n    valid_sequences, valid_targets = sequences[train_size:train_size + valid_size], targets[train_size:train_size + valid_size]\n    test_sequences, test_targets = sequences[train_size + valid_size:], targets[train_size + valid_size:]\n\n    return train_sequences, train_targets, valid_sequences, valid_targets, test_sequences, test_targets",
   "metadata": {
    "id": "Hq_1Kc9Vmqp2",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:08:44.376658Z",
     "iopub.execute_input": "2025-05-31T16:08:44.377226Z",
     "iopub.status.idle": "2025-05-31T16:08:44.381921Z",
     "shell.execute_reply.started": "2025-05-31T16:08:44.377206Z",
     "shell.execute_reply": "2025-05-31T16:08:44.381207Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "train_sequences, train_targets, valid_sequences, valid_targets, test_sequences, test_targets = split_data(sequences, targets)",
   "metadata": {
    "id": "rX0ZC4acmsaV",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:08:47.257828Z",
     "iopub.execute_input": "2025-05-31T16:08:47.258109Z",
     "iopub.status.idle": "2025-05-31T16:08:47.263605Z",
     "shell.execute_reply.started": "2025-05-31T16:08:47.258089Z",
     "shell.execute_reply": "2025-05-31T16:08:47.262799Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "def collate(batch):\n    inputs, targets = zip(*batch)\n    inputs_padded = pad_sequences(inputs, padding_val=token2idx['<PAD>'])\n    return torch.LongTensor(inputs_padded), torch.LongTensor(targets)",
   "metadata": {
    "id": "BoS4k2wwmzaN",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:08:47.952459Z",
     "iopub.execute_input": "2025-05-31T16:08:47.952927Z",
     "iopub.status.idle": "2025-05-31T16:08:47.957011Z",
     "shell.execute_reply.started": "2025-05-31T16:08:47.952905Z",
     "shell.execute_reply": "2025-05-31T16:08:47.956218Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "def pad_sequences(sequences, padding_val=0, pad_left=False):\n    \"\"\"Pad a list of sequences to the same length with a padding_val.\"\"\"\n    sequence_length = max(len(sequence) for sequence in sequences)\n    if not pad_left:\n        return [sequence + [padding_val] * (sequence_length - len(sequence)) for sequence in sequences]\n    return [[padding_val] * (sequence_length - len(sequence)) + sequence for sequence in sequences]",
   "metadata": {
    "id": "D_29NGAJnDo8",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:08:48.749215Z",
     "iopub.execute_input": "2025-05-31T16:08:48.749584Z",
     "iopub.status.idle": "2025-05-31T16:08:48.754303Z",
     "shell.execute_reply.started": "2025-05-31T16:08:48.749561Z",
     "shell.execute_reply": "2025-05-31T16:08:48.753536Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "batch_size = 256\ntrain_data = list(zip(train_sequences, train_targets))\nvalid_data = list(zip(valid_sequences, valid_targets))\ntest_data = list(zip(test_sequences, test_targets))\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate)\nvalid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False, collate_fn=collate)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate)",
   "metadata": {
    "id": "HB33EytF1QCW",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:08:49.255479Z",
     "iopub.execute_input": "2025-05-31T16:08:49.256055Z",
     "iopub.status.idle": "2025-05-31T16:08:49.487529Z",
     "shell.execute_reply.started": "2025-05-31T16:08:49.256033Z",
     "shell.execute_reply": "2025-05-31T16:08:49.486792Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": "# RNN section",
   "metadata": {
    "id": "qjwZOOd6_E1J"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### RNN with nn.RNN\nImplement a basic RNN model using PyTorch's built-in nn.RNN.\nDefine layers: embedding, RNN, and fully connected.",
   "metadata": {
    "id": "-EB2IPhH_fDR"
   }
  },
  {
   "cell_type": "code",
   "source": "class RNNClassifier(nn.Module):\n    def __init__(self, output_size, hidden_size, vocab_size,\n                 device=device, n_layers=1,\n                 embedding_dimension=50):\n        super(RNNClassifier, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.device = device\n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dimension, padding_idx=token2idx['<PAD>'])\n\n        ###################################### TODO #####################################\n        #                          COMPLETE THE FOLLOWING SECTION                       #\n        #################################################################################\n        \"\"\"Define Needed Layers \"\"\"\n        \n        self.rnn = nn.RNN(embedding_dimension, hidden_size, n_layers, batch_first=True)\n        \n        self.fc = nn.Linear(hidden_size, output_size)\n        \n        #################################################################################\n        #                                   THE END                                     #\n        #################################################################################\n\n    def forward(self, inputs):\n        ###################################### TODO #####################################\n        #                          COMPLETE THE FOLLOWING SECTION                       #\n        #################################################################################\n                \"\"\"\n                Implements the forward pass: first, embed the input tokens, then pass\n                the embeddings through the RNN layer to capture sequential dependencies.\n                Finally, use fully connected layers to output class probabilities.\n                \"\"\"\n                x = self.embedding(inputs)\n                h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_size).to(x.device)\n\n                out, _ = self.rnn(x, h0)\n                # out: batch_size, seq_length, hidden_size\n\n                out = out[:, -1, :]\n                out = self.fc(out)\n\n\n        #################################################################################\n        #                                   THE END                                     #\n        #################################################################################\n                return out # probabilities for each class in the output.",
   "metadata": {
    "id": "KR4ggfh8nK3d",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:08:51.661123Z",
     "iopub.execute_input": "2025-05-31T16:08:51.661805Z",
     "iopub.status.idle": "2025-05-31T16:08:51.667867Z",
     "shell.execute_reply.started": "2025-05-31T16:08:51.661783Z",
     "shell.execute_reply": "2025-05-31T16:08:51.667197Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": "### Train model\n\nIn this section, you should train model for multiple epochs on the training data and evaluate it on the validation data after each epoch, reporting the model's accuracy. Ensure that the model is set to train mode during training and switched to eval mode for evaluation on the validation data. The objective is to implement the training loop and, at the next , compute and report the final accuracy on the test data.\n\n**Note**: You are not allowed to use library-built trainer functions in this section; the training loop should be implemented manually.\n\n**Note**: To implement the training loop, you have the option to create a single train_model function that trains a model over multiple epochs, calculates training and validation accuracy, and logs the losses. Once written, this function can be reused for all RNN and LSTM models, allowing you to simply call it with different model instances for training. Reusing the function in this way will ensure that you receive credit for the training section of each subsequent model without needing to write separate loops , with just the correct function call.\n\n\n\n\n\n\n",
   "metadata": {
    "id": "2Xv9SWFfFPjA"
   }
  },
  {
   "cell_type": "code",
   "source": "# TODO: edit this method\ndef train_model(model,\n                train_loader,\n                valid_loader,\n                criterion,\n                optimizer,\n                n_epochs,\n                device=device):\n    \"\"\"\n    Trains `model` for n_epochs, evaluates on valid_loader each epoch,\n    and finally reports accuracy on test_loader.\n    \"\"\"\n    model.to(device)\n    \n    for epoch in range(1, n_epochs+1):\n        # ————— Training —————\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n        \n        for inputs, targets in train_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)                      # forward pass\n            loss = criterion(outputs, targets)           # compute loss\n            loss.backward()                              # backprop\n            optimizer.step()                             # gradient step\n            \n            # accumulate stats\n            train_loss += loss.item() * inputs.size(0)\n            _, preds = torch.max(outputs, dim=1)\n            train_correct += (preds == targets).sum().item()\n            train_total += targets.size(0)\n        \n        train_loss /= train_total\n        train_acc = train_correct / train_total\n        \n        # ————— Validation —————\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for inputs, targets in valid_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                \n                val_loss += loss.item() * inputs.size(0)\n                _, preds = torch.max(outputs, dim=1)\n                val_correct += (preds == targets).sum().item()\n                val_total += targets.size(0)\n        \n        val_loss /= val_total\n        val_acc = val_correct / val_total\n        \n        print(f\"Epoch {epoch}/{n_epochs}  \"\n              f\"Train Loss: {train_loss:.4f}  Train Acc: {train_acc:.4f}  |  \"\n              f\"Val Loss: {val_loss:.4f}  Val Acc: {val_acc:.4f}\")\n    \n    return model\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:08:54.846289Z",
     "iopub.execute_input": "2025-05-31T16:08:54.846567Z",
     "iopub.status.idle": "2025-05-31T16:08:54.853977Z",
     "shell.execute_reply.started": "2025-05-31T16:08:54.846546Z",
     "shell.execute_reply": "2025-05-31T16:08:54.853294Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "# Binary classification problem\nnum_classes = 2\n\n# the model underfits when hidden_dim=128, so we use 256 instead.\nhidden_dim = 256\n\n# num_layers seems to have little or no effect on accuracy. the model does fine even if this hyperparameter is set to 1.\nrnn = RNNClassifier(num_classes, hidden_dim, len(token2idx), n_layers=1)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(rnn.parameters(), lr=3e-5)",
   "metadata": {
    "id": "a2orLI-qXeFW",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:09:02.984718Z",
     "iopub.execute_input": "2025-05-31T16:09:02.984970Z",
     "iopub.status.idle": "2025-05-31T16:09:05.555666Z",
     "shell.execute_reply.started": "2025-05-31T16:09:02.984952Z",
     "shell.execute_reply": "2025-05-31T16:09:05.555107Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": "############################# TODO #############################\n# TODO: Implement the training loop\n################################################################\n\n\nn_epochs  = 100\n\nrnn = train_model(\n    rnn,\n    train_loader,\n    valid_loader,\n    criterion,\n    optimizer,\n    n_epochs,\n    device\n)\n",
   "metadata": {
    "id": "CFw6Wl7xFW7m",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:09:05.556714Z",
     "iopub.execute_input": "2025-05-31T16:09:05.557174Z",
     "iopub.status.idle": "2025-05-31T16:12:15.897407Z",
     "shell.execute_reply.started": "2025-05-31T16:09:05.557156Z",
     "shell.execute_reply": "2025-05-31T16:12:15.896638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/100  Train Loss: 0.6879  Train Acc: 0.5544  |  Val Loss: 0.8103  Val Acc: 0.0584\nEpoch 2/100  Train Loss: 0.6833  Train Acc: 0.5630  |  Val Loss: 0.8062  Val Acc: 0.0800\nEpoch 3/100  Train Loss: 0.6807  Train Acc: 0.5677  |  Val Loss: 0.7975  Val Acc: 0.1044\nEpoch 4/100  Train Loss: 0.6781  Train Acc: 0.5724  |  Val Loss: 0.8162  Val Acc: 0.1248\nEpoch 5/100  Train Loss: 0.6619  Train Acc: 0.5997  |  Val Loss: 0.7973  Val Acc: 0.3485\nEpoch 6/100  Train Loss: 0.6424  Train Acc: 0.6316  |  Val Loss: 0.6975  Val Acc: 0.5634\nEpoch 7/100  Train Loss: 0.6349  Train Acc: 0.6455  |  Val Loss: 0.7084  Val Acc: 0.5978\nEpoch 8/100  Train Loss: 0.6269  Train Acc: 0.6585  |  Val Loss: 0.6599  Val Acc: 0.6631\nEpoch 9/100  Train Loss: 0.6228  Train Acc: 0.6649  |  Val Loss: 0.7876  Val Acc: 0.5250\nEpoch 10/100  Train Loss: 0.6189  Train Acc: 0.6699  |  Val Loss: 0.6781  Val Acc: 0.6535\nEpoch 11/100  Train Loss: 0.6162  Train Acc: 0.6729  |  Val Loss: 0.6999  Val Acc: 0.6214\nEpoch 12/100  Train Loss: 0.6144  Train Acc: 0.6737  |  Val Loss: 0.7241  Val Acc: 0.6010\nEpoch 13/100  Train Loss: 0.6092  Train Acc: 0.6791  |  Val Loss: 0.7140  Val Acc: 0.6198\nEpoch 14/100  Train Loss: 0.6099  Train Acc: 0.6775  |  Val Loss: 0.6602  Val Acc: 0.6991\nEpoch 15/100  Train Loss: 0.6056  Train Acc: 0.6845  |  Val Loss: 0.6347  Val Acc: 0.6991\nEpoch 16/100  Train Loss: 0.6019  Train Acc: 0.6886  |  Val Loss: 0.7099  Val Acc: 0.6339\nEpoch 17/100  Train Loss: 0.6021  Train Acc: 0.6851  |  Val Loss: 0.8550  Val Acc: 0.5166\nEpoch 18/100  Train Loss: 0.5972  Train Acc: 0.6907  |  Val Loss: 0.6646  Val Acc: 0.6943\nEpoch 19/100  Train Loss: 0.5954  Train Acc: 0.6930  |  Val Loss: 0.4933  Val Acc: 0.8439\nEpoch 20/100  Train Loss: 0.6003  Train Acc: 0.6864  |  Val Loss: 0.8475  Val Acc: 0.4662\nEpoch 21/100  Train Loss: 0.5909  Train Acc: 0.6988  |  Val Loss: 0.6972  Val Acc: 0.6535\nEpoch 22/100  Train Loss: 0.5893  Train Acc: 0.7008  |  Val Loss: 0.6794  Val Acc: 0.6767\nEpoch 23/100  Train Loss: 0.5875  Train Acc: 0.7028  |  Val Loss: 0.6591  Val Acc: 0.6875\nEpoch 24/100  Train Loss: 0.5865  Train Acc: 0.7035  |  Val Loss: 0.7637  Val Acc: 0.5882\nEpoch 25/100  Train Loss: 0.5823  Train Acc: 0.7055  |  Val Loss: 0.7176  Val Acc: 0.6319\nEpoch 26/100  Train Loss: 0.5793  Train Acc: 0.7096  |  Val Loss: 0.7615  Val Acc: 0.6026\nEpoch 27/100  Train Loss: 0.5780  Train Acc: 0.7120  |  Val Loss: 0.6325  Val Acc: 0.7099\nEpoch 28/100  Train Loss: 0.5763  Train Acc: 0.7113  |  Val Loss: 0.6710  Val Acc: 0.6767\nEpoch 29/100  Train Loss: 0.5735  Train Acc: 0.7144  |  Val Loss: 0.6582  Val Acc: 0.6935\nEpoch 30/100  Train Loss: 0.5735  Train Acc: 0.7163  |  Val Loss: 0.8290  Val Acc: 0.5338\nEpoch 31/100  Train Loss: 0.5698  Train Acc: 0.7178  |  Val Loss: 0.7523  Val Acc: 0.6154\nEpoch 32/100  Train Loss: 0.5672  Train Acc: 0.7217  |  Val Loss: 0.6193  Val Acc: 0.7147\nEpoch 33/100  Train Loss: 0.5669  Train Acc: 0.7203  |  Val Loss: 0.6434  Val Acc: 0.6983\nEpoch 34/100  Train Loss: 0.5625  Train Acc: 0.7250  |  Val Loss: 0.7077  Val Acc: 0.6599\nEpoch 35/100  Train Loss: 0.5607  Train Acc: 0.7263  |  Val Loss: 0.6734  Val Acc: 0.6643\nEpoch 36/100  Train Loss: 0.5580  Train Acc: 0.7271  |  Val Loss: 0.7369  Val Acc: 0.6355\nEpoch 37/100  Train Loss: 0.5561  Train Acc: 0.7302  |  Val Loss: 0.6035  Val Acc: 0.7275\nEpoch 38/100  Train Loss: 0.5537  Train Acc: 0.7308  |  Val Loss: 0.6382  Val Acc: 0.7115\nEpoch 39/100  Train Loss: 0.5529  Train Acc: 0.7359  |  Val Loss: 0.6588  Val Acc: 0.6727\nEpoch 40/100  Train Loss: 0.5530  Train Acc: 0.7337  |  Val Loss: 0.6728  Val Acc: 0.6727\nEpoch 41/100  Train Loss: 0.5467  Train Acc: 0.7379  |  Val Loss: 0.6121  Val Acc: 0.7243\nEpoch 42/100  Train Loss: 0.5462  Train Acc: 0.7400  |  Val Loss: 0.6792  Val Acc: 0.6555\nEpoch 43/100  Train Loss: 0.5446  Train Acc: 0.7392  |  Val Loss: 0.6556  Val Acc: 0.6883\nEpoch 44/100  Train Loss: 0.5427  Train Acc: 0.7411  |  Val Loss: 0.6961  Val Acc: 0.6507\nEpoch 45/100  Train Loss: 0.5400  Train Acc: 0.7424  |  Val Loss: 0.6829  Val Acc: 0.6595\nEpoch 46/100  Train Loss: 0.5359  Train Acc: 0.7453  |  Val Loss: 0.6723  Val Acc: 0.6739\nEpoch 47/100  Train Loss: 0.5331  Train Acc: 0.7480  |  Val Loss: 0.6542  Val Acc: 0.6903\nEpoch 48/100  Train Loss: 0.5315  Train Acc: 0.7482  |  Val Loss: 0.6969  Val Acc: 0.6611\nEpoch 49/100  Train Loss: 0.5312  Train Acc: 0.7485  |  Val Loss: 0.7184  Val Acc: 0.6527\nEpoch 50/100  Train Loss: 0.5287  Train Acc: 0.7504  |  Val Loss: 0.6315  Val Acc: 0.7119\nEpoch 51/100  Train Loss: 0.5239  Train Acc: 0.7546  |  Val Loss: 0.6261  Val Acc: 0.7175\nEpoch 52/100  Train Loss: 0.5222  Train Acc: 0.7558  |  Val Loss: 0.5760  Val Acc: 0.7527\nEpoch 53/100  Train Loss: 0.5218  Train Acc: 0.7552  |  Val Loss: 0.7280  Val Acc: 0.6463\nEpoch 54/100  Train Loss: 0.5166  Train Acc: 0.7584  |  Val Loss: 0.6889  Val Acc: 0.6639\nEpoch 55/100  Train Loss: 0.5145  Train Acc: 0.7589  |  Val Loss: 0.5970  Val Acc: 0.7375\nEpoch 56/100  Train Loss: 0.5181  Train Acc: 0.7572  |  Val Loss: 0.5713  Val Acc: 0.7495\nEpoch 57/100  Train Loss: 0.5090  Train Acc: 0.7615  |  Val Loss: 0.7139  Val Acc: 0.6363\nEpoch 58/100  Train Loss: 0.5053  Train Acc: 0.7628  |  Val Loss: 0.6686  Val Acc: 0.6915\nEpoch 59/100  Train Loss: 0.4947  Train Acc: 0.7643  |  Val Loss: 0.5483  Val Acc: 0.7467\nEpoch 60/100  Train Loss: 0.4914  Train Acc: 0.7648  |  Val Loss: 0.6589  Val Acc: 0.6883\nEpoch 61/100  Train Loss: 0.4850  Train Acc: 0.7696  |  Val Loss: 0.4615  Val Acc: 0.7915\nEpoch 62/100  Train Loss: 0.4845  Train Acc: 0.7694  |  Val Loss: 0.5929  Val Acc: 0.6927\nEpoch 63/100  Train Loss: 0.4804  Train Acc: 0.7729  |  Val Loss: 0.5266  Val Acc: 0.7639\nEpoch 64/100  Train Loss: 0.4758  Train Acc: 0.7754  |  Val Loss: 0.5609  Val Acc: 0.7375\nEpoch 65/100  Train Loss: 0.4697  Train Acc: 0.7781  |  Val Loss: 0.4444  Val Acc: 0.7967\nEpoch 66/100  Train Loss: 0.4680  Train Acc: 0.7784  |  Val Loss: 0.5085  Val Acc: 0.7675\nEpoch 67/100  Train Loss: 0.4648  Train Acc: 0.7800  |  Val Loss: 0.6991  Val Acc: 0.6511\nEpoch 68/100  Train Loss: 0.4612  Train Acc: 0.7831  |  Val Loss: 0.5222  Val Acc: 0.7371\nEpoch 69/100  Train Loss: 0.4585  Train Acc: 0.7844  |  Val Loss: 0.5697  Val Acc: 0.7027\nEpoch 70/100  Train Loss: 0.4534  Train Acc: 0.7874  |  Val Loss: 0.5034  Val Acc: 0.7531\nEpoch 71/100  Train Loss: 0.4508  Train Acc: 0.7889  |  Val Loss: 0.4992  Val Acc: 0.7587\nEpoch 72/100  Train Loss: 0.4485  Train Acc: 0.7901  |  Val Loss: 0.6336  Val Acc: 0.6715\nEpoch 73/100  Train Loss: 0.4435  Train Acc: 0.7926  |  Val Loss: 0.5858  Val Acc: 0.7071\nEpoch 74/100  Train Loss: 0.4412  Train Acc: 0.7943  |  Val Loss: 0.5436  Val Acc: 0.7323\nEpoch 75/100  Train Loss: 0.4377  Train Acc: 0.7973  |  Val Loss: 0.5372  Val Acc: 0.7479\nEpoch 76/100  Train Loss: 0.4352  Train Acc: 0.7974  |  Val Loss: 0.4778  Val Acc: 0.7763\nEpoch 77/100  Train Loss: 0.4313  Train Acc: 0.8004  |  Val Loss: 0.3985  Val Acc: 0.8171\nEpoch 78/100  Train Loss: 0.4285  Train Acc: 0.8012  |  Val Loss: 0.4906  Val Acc: 0.7679\nEpoch 79/100  Train Loss: 0.4260  Train Acc: 0.8034  |  Val Loss: 0.4842  Val Acc: 0.7643\nEpoch 80/100  Train Loss: 0.4238  Train Acc: 0.8042  |  Val Loss: 0.4554  Val Acc: 0.7827\nEpoch 81/100  Train Loss: 0.4218  Train Acc: 0.8054  |  Val Loss: 0.7142  Val Acc: 0.6263\nEpoch 82/100  Train Loss: 0.4177  Train Acc: 0.8089  |  Val Loss: 0.5668  Val Acc: 0.7223\nEpoch 83/100  Train Loss: 0.4152  Train Acc: 0.8099  |  Val Loss: 0.4979  Val Acc: 0.7663\nEpoch 84/100  Train Loss: 0.4108  Train Acc: 0.8125  |  Val Loss: 0.5080  Val Acc: 0.7515\nEpoch 85/100  Train Loss: 0.4089  Train Acc: 0.8124  |  Val Loss: 0.4870  Val Acc: 0.7591\nEpoch 86/100  Train Loss: 0.4111  Train Acc: 0.8117  |  Val Loss: 0.5232  Val Acc: 0.7691\nEpoch 87/100  Train Loss: 0.4050  Train Acc: 0.8166  |  Val Loss: 0.5374  Val Acc: 0.7435\nEpoch 88/100  Train Loss: 0.4054  Train Acc: 0.8162  |  Val Loss: 0.4208  Val Acc: 0.8083\nEpoch 89/100  Train Loss: 0.4024  Train Acc: 0.8168  |  Val Loss: 0.5474  Val Acc: 0.7403\nEpoch 90/100  Train Loss: 0.3977  Train Acc: 0.8204  |  Val Loss: 0.5693  Val Acc: 0.7135\nEpoch 91/100  Train Loss: 0.3935  Train Acc: 0.8211  |  Val Loss: 0.4628  Val Acc: 0.7927\nEpoch 92/100  Train Loss: 0.3952  Train Acc: 0.8222  |  Val Loss: 0.5150  Val Acc: 0.7531\nEpoch 93/100  Train Loss: 0.3910  Train Acc: 0.8242  |  Val Loss: 0.6464  Val Acc: 0.6527\nEpoch 94/100  Train Loss: 0.3910  Train Acc: 0.8240  |  Val Loss: 0.5381  Val Acc: 0.7379\nEpoch 95/100  Train Loss: 0.3856  Train Acc: 0.8286  |  Val Loss: 0.4381  Val Acc: 0.8007\nEpoch 96/100  Train Loss: 0.3872  Train Acc: 0.8249  |  Val Loss: 0.4636  Val Acc: 0.7647\nEpoch 97/100  Train Loss: 0.3829  Train Acc: 0.8277  |  Val Loss: 0.4521  Val Acc: 0.8051\nEpoch 98/100  Train Loss: 0.3804  Train Acc: 0.8291  |  Val Loss: 0.3445  Val Acc: 0.8411\nEpoch 99/100  Train Loss: 0.3781  Train Acc: 0.8297  |  Val Loss: 0.4867  Val Acc: 0.7683\nEpoch 100/100  Train Loss: 0.3767  Train Acc: 0.8312  |  Val Loss: 0.3641  Val Acc: 0.8571\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": "### RNN from Scratch\nImplement an RNN from scratch by creating a custom RNN cell and a model that stacks these cells over time.",
   "metadata": {
    "id": "TJpjgTNc_rdx"
   }
  },
  {
   "cell_type": "code",
   "source": "class CustomRNNCell(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(CustomRNNCell, self).__init__()\n        self.hidden_size = hidden_size\n        ###################################### TODO #####################################\n        #                          COMPLETE THE FOLLOWING SECTION                       #\n        #################################################################################\n                # \"Define Input-to-Hidden and Hidden-to-Hidden Layers\"\"\"\n        self.input_size = input_size\n\n        self.x2h = nn.Linear(input_size, hidden_size, bias=True)\n        self.h2h = nn.Linear(hidden_size, hidden_size, bias=True)\n        \n\n        #################################################################################\n        #                                   THE END                                     #\n        #################################################################################\n\n    def forward(self, input, hidden):\n        ###################################### TODO #####################################\n        #                          COMPLETE THE FOLLOWING SECTION                       #\n        #################################################################################\n                \"\"\"\n                Implements the forward pass: combines the input and previous hidden state\n                to calculate the new hidden state for this RNN cell.\n                \n                # Inputs:\n                #       input: of shape (batch_size, input_size)\n                #       hidden: of shape (batch_size, hidden_size)\n                # Output:\n                #       hidden: of shape (batch_size, hidden_size)\n                \"\"\"\n                if hidden is None:\n                    hidden = input.new_zeros(input.size(0), self.hidden_size)\n        \n                hidden = (self.x2h(input) + self.h2h(hidden))\n        \n                hidden = torch.tanh(hidden)\n\n\n        #################################################################################\n        #                                   THE END                                     #\n        #################################################################################\n                return hidden\n",
   "metadata": {
    "id": "BjJfWY4zfsBR",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:12:21.839960Z",
     "iopub.execute_input": "2025-05-31T16:12:21.840443Z",
     "iopub.status.idle": "2025-05-31T16:12:21.845997Z",
     "shell.execute_reply.started": "2025-05-31T16:12:21.840419Z",
     "shell.execute_reply": "2025-05-31T16:12:21.845218Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": "class CustomRNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, output_size):\n        super(CustomRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=token2idx['<PAD>'])\n        ###################################### TODO #####################################\n        #                          COMPLETE THE FOLLOWING SECTION                       #\n        #################################################################################\n                # \"\"\"Define Custom RNN Cell and Fully Connected Layers\"\"\"\n        self.input_size = embedding_dim\n        self.num_layers = num_layers\n        self.output_size = output_size\n\n        self.rnn_cell_list = nn.ModuleList()\n\n        self.rnn_cell_list.append(CustomRNNCell(self.input_size, self.hidden_size))\n        for l in range(1, self.num_layers):\n            self.rnn_cell_list.append(CustomRNNCell(self.hidden_size, self.hidden_size))\n\n        self.fc = nn.Linear(self.hidden_size, self.output_size)\n\n        #################################################################################\n        #                                   THE END                                     #\n        #################################################################################\n\n\n    def forward(self, inputs):\n        ###################################### TODO #####################################\n        #                          COMPLETE THE FOLLOWING SECTION                       #\n        #################################################################################\n                # \"\"\"\n                # Implements the forward pass: performs embedding lookup, iterates through each\n                # time step, and passes embeddings through the custom RNN cell. Finally,\n                # applies the fully connected layers to output class probabilities.\n\n                # # Input of shape (batch_size, seqence length, input_size)\n                # #\n                # # Output of shape (batch_size, output_size)\n                # \"\"\"\n\n        x = self.embedding(inputs)\n    \n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n\n        outs = []\n\n        hidden = list()\n        for layer in range(self.num_layers):\n            hidden.append(h0[layer, :, :])\n\n        for t in range(x.size(1)):\n\n            for layer in range(self.num_layers):\n\n                if layer == 0:\n                    hidden_l = self.rnn_cell_list[layer](x[:, t, :], hidden[layer])\n                else:\n                    hidden_l = self.rnn_cell_list[layer](hidden[layer - 1],hidden[layer])\n                hidden[layer] = hidden_l\n\n                hidden[layer] = hidden_l\n\n            outs.append(hidden_l)\n\n        # Take only last time step. Modify for seq to seq\n        out = outs[-1]\n\n        out = self.fc(out)\n        \n\n        #################################################################################\n        #                                   THE END                                     #\n        #################################################################################\n        return out # probabilities for each class in the output.",
   "metadata": {
    "id": "UPODDCcAft1a",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:12:23.605401Z",
     "iopub.execute_input": "2025-05-31T16:12:23.605915Z",
     "iopub.status.idle": "2025-05-31T16:12:23.614016Z",
     "shell.execute_reply.started": "2025-05-31T16:12:23.605893Z",
     "shell.execute_reply": "2025-05-31T16:12:23.613315Z"
    }
   },
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": "### Train model\n\nIn this section, you should train model for multiple epochs on the training data and evaluate it on the validation data after each epoch, reporting the model's accuracy. Ensure that the model is set to train mode during training and switched to eval mode for evaluation on the validation data. The objective is to implement the training loop and, at the next , compute and report the final accuracy on the test data.",
   "metadata": {
    "id": "jMTPh0---Y63"
   }
  },
  {
   "cell_type": "code",
   "source": "vocab_size = len(token2idx)\nembedding_dim = 50\nhidden_size = 256\nnum_layers = 1\noutput_size = 2\n\n\ncustomRNN = CustomRNN(vocab_size, embedding_dim , hidden_size, num_layers, output_size)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(customRNN.parameters(), lr=2e-5)",
   "metadata": {
    "id": "nRgR4YxHfv5w",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:12:59.402638Z",
     "iopub.execute_input": "2025-05-31T16:12:59.403385Z",
     "iopub.status.idle": "2025-05-31T16:12:59.410050Z",
     "shell.execute_reply.started": "2025-05-31T16:12:59.403361Z",
     "shell.execute_reply": "2025-05-31T16:12:59.409355Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": "############################# TODO #############################\n# TODO: Implement the training loop\n################################################################\n\n\nn_epochs  = 100\n\ncustomRNN = train_model(\n    customRNN,\n    train_loader,\n    valid_loader,\n    criterion,\n    optimizer,\n    n_epochs,\n    device\n)\n",
   "metadata": {
    "id": "GH_uYgXTCziw",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:13:07.215710Z",
     "iopub.execute_input": "2025-05-31T16:13:07.215978Z",
     "iopub.status.idle": "2025-05-31T16:26:01.300335Z",
     "shell.execute_reply.started": "2025-05-31T16:13:07.215959Z",
     "shell.execute_reply": "2025-05-31T16:26:01.299531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/100  Train Loss: 0.6911  Train Acc: 0.5480  |  Val Loss: 0.8034  Val Acc: 0.0972\nEpoch 2/100  Train Loss: 0.6862  Train Acc: 0.5570  |  Val Loss: 0.8173  Val Acc: 0.0952\nEpoch 3/100  Train Loss: 0.6840  Train Acc: 0.5634  |  Val Loss: 0.7957  Val Acc: 0.1084\nEpoch 4/100  Train Loss: 0.6823  Train Acc: 0.5649  |  Val Loss: 0.8116  Val Acc: 0.1040\nEpoch 5/100  Train Loss: 0.6810  Train Acc: 0.5679  |  Val Loss: 0.8110  Val Acc: 0.1088\nEpoch 6/100  Train Loss: 0.6799  Train Acc: 0.5699  |  Val Loss: 0.8094  Val Acc: 0.1184\nEpoch 7/100  Train Loss: 0.6787  Train Acc: 0.5713  |  Val Loss: 0.8082  Val Acc: 0.1196\nEpoch 8/100  Train Loss: 0.6776  Train Acc: 0.5730  |  Val Loss: 0.8030  Val Acc: 0.1325\nEpoch 9/100  Train Loss: 0.6765  Train Acc: 0.5745  |  Val Loss: 0.8118  Val Acc: 0.1341\nEpoch 10/100  Train Loss: 0.6743  Train Acc: 0.5769  |  Val Loss: 0.8024  Val Acc: 0.2689\nEpoch 11/100  Train Loss: 0.6643  Train Acc: 0.5989  |  Val Loss: 0.7889  Val Acc: 0.3585\nEpoch 12/100  Train Loss: 0.6593  Train Acc: 0.6050  |  Val Loss: 0.7648  Val Acc: 0.4146\nEpoch 13/100  Train Loss: 0.6557  Train Acc: 0.6092  |  Val Loss: 0.7931  Val Acc: 0.3826\nEpoch 14/100  Train Loss: 0.6529  Train Acc: 0.6134  |  Val Loss: 0.7529  Val Acc: 0.4246\nEpoch 15/100  Train Loss: 0.6508  Train Acc: 0.6169  |  Val Loss: 0.8189  Val Acc: 0.3573\nEpoch 16/100  Train Loss: 0.6488  Train Acc: 0.6197  |  Val Loss: 0.8113  Val Acc: 0.3842\nEpoch 17/100  Train Loss: 0.6469  Train Acc: 0.6226  |  Val Loss: 0.7894  Val Acc: 0.4022\nEpoch 18/100  Train Loss: 0.6451  Train Acc: 0.6247  |  Val Loss: 0.7528  Val Acc: 0.4398\nEpoch 19/100  Train Loss: 0.6429  Train Acc: 0.6272  |  Val Loss: 0.7890  Val Acc: 0.4134\nEpoch 20/100  Train Loss: 0.6412  Train Acc: 0.6291  |  Val Loss: 0.7929  Val Acc: 0.4006\nEpoch 21/100  Train Loss: 0.6392  Train Acc: 0.6311  |  Val Loss: 0.7540  Val Acc: 0.4646\nEpoch 22/100  Train Loss: 0.6372  Train Acc: 0.6343  |  Val Loss: 0.7353  Val Acc: 0.4834\nEpoch 23/100  Train Loss: 0.6356  Train Acc: 0.6358  |  Val Loss: 0.7741  Val Acc: 0.4454\nEpoch 24/100  Train Loss: 0.6336  Train Acc: 0.6386  |  Val Loss: 0.7219  Val Acc: 0.5034\nEpoch 25/100  Train Loss: 0.6318  Train Acc: 0.6421  |  Val Loss: 0.7652  Val Acc: 0.4618\nEpoch 26/100  Train Loss: 0.6299  Train Acc: 0.6435  |  Val Loss: 0.7217  Val Acc: 0.5106\nEpoch 27/100  Train Loss: 0.6284  Train Acc: 0.6457  |  Val Loss: 0.7171  Val Acc: 0.5074\nEpoch 28/100  Train Loss: 0.6261  Train Acc: 0.6469  |  Val Loss: 0.7257  Val Acc: 0.5114\nEpoch 29/100  Train Loss: 0.6240  Train Acc: 0.6505  |  Val Loss: 0.7204  Val Acc: 0.5166\nEpoch 30/100  Train Loss: 0.6214  Train Acc: 0.6526  |  Val Loss: 0.7176  Val Acc: 0.5146\nEpoch 31/100  Train Loss: 0.6193  Train Acc: 0.6548  |  Val Loss: 0.7009  Val Acc: 0.5334\nEpoch 32/100  Train Loss: 0.6175  Train Acc: 0.6565  |  Val Loss: 0.7424  Val Acc: 0.5198\nEpoch 33/100  Train Loss: 0.6157  Train Acc: 0.6602  |  Val Loss: 0.7056  Val Acc: 0.5426\nEpoch 34/100  Train Loss: 0.6128  Train Acc: 0.6633  |  Val Loss: 0.6939  Val Acc: 0.5490\nEpoch 35/100  Train Loss: 0.6102  Train Acc: 0.6669  |  Val Loss: 0.7411  Val Acc: 0.5110\nEpoch 36/100  Train Loss: 0.6082  Train Acc: 0.6679  |  Val Loss: 0.7828  Val Acc: 0.4950\nEpoch 37/100  Train Loss: 0.6040  Train Acc: 0.6735  |  Val Loss: 0.6925  Val Acc: 0.5654\nEpoch 38/100  Train Loss: 0.6009  Train Acc: 0.6745  |  Val Loss: 0.7015  Val Acc: 0.5418\nEpoch 39/100  Train Loss: 0.5977  Train Acc: 0.6782  |  Val Loss: 0.8449  Val Acc: 0.4390\nEpoch 40/100  Train Loss: 0.5949  Train Acc: 0.6823  |  Val Loss: 0.7013  Val Acc: 0.5602\nEpoch 41/100  Train Loss: 0.5909  Train Acc: 0.6860  |  Val Loss: 0.8636  Val Acc: 0.4854\nEpoch 42/100  Train Loss: 0.5871  Train Acc: 0.6891  |  Val Loss: 0.6810  Val Acc: 0.5794\nEpoch 43/100  Train Loss: 0.5801  Train Acc: 0.6949  |  Val Loss: 0.5944  Val Acc: 0.6579\nEpoch 44/100  Train Loss: 0.5769  Train Acc: 0.6987  |  Val Loss: 0.6031  Val Acc: 0.6499\nEpoch 45/100  Train Loss: 0.5719  Train Acc: 0.7028  |  Val Loss: 0.6753  Val Acc: 0.5958\nEpoch 46/100  Train Loss: 0.5666  Train Acc: 0.7097  |  Val Loss: 0.7078  Val Acc: 0.5798\nEpoch 47/100  Train Loss: 0.5632  Train Acc: 0.7126  |  Val Loss: 0.7750  Val Acc: 0.5330\nEpoch 48/100  Train Loss: 0.5583  Train Acc: 0.7155  |  Val Loss: 0.7374  Val Acc: 0.5670\nEpoch 49/100  Train Loss: 0.5564  Train Acc: 0.7195  |  Val Loss: 0.5893  Val Acc: 0.6827\nEpoch 50/100  Train Loss: 0.5520  Train Acc: 0.7226  |  Val Loss: 0.7273  Val Acc: 0.5630\nEpoch 51/100  Train Loss: 0.5452  Train Acc: 0.7283  |  Val Loss: 0.6801  Val Acc: 0.6230\nEpoch 52/100  Train Loss: 0.5403  Train Acc: 0.7323  |  Val Loss: 0.6082  Val Acc: 0.6891\nEpoch 53/100  Train Loss: 0.5372  Train Acc: 0.7362  |  Val Loss: 0.7030  Val Acc: 0.6102\nEpoch 54/100  Train Loss: 0.5337  Train Acc: 0.7409  |  Val Loss: 0.6310  Val Acc: 0.6583\nEpoch 55/100  Train Loss: 0.5285  Train Acc: 0.7454  |  Val Loss: 0.7307  Val Acc: 0.6034\nEpoch 56/100  Train Loss: 0.5233  Train Acc: 0.7491  |  Val Loss: 0.6619  Val Acc: 0.6539\nEpoch 57/100  Train Loss: 0.5209  Train Acc: 0.7516  |  Val Loss: 0.5733  Val Acc: 0.7143\nEpoch 58/100  Train Loss: 0.5183  Train Acc: 0.7545  |  Val Loss: 0.5748  Val Acc: 0.7159\nEpoch 59/100  Train Loss: 0.5158  Train Acc: 0.7571  |  Val Loss: 0.5037  Val Acc: 0.7611\nEpoch 60/100  Train Loss: 0.5124  Train Acc: 0.7595  |  Val Loss: 0.5400  Val Acc: 0.7523\nEpoch 61/100  Train Loss: 0.5086  Train Acc: 0.7628  |  Val Loss: 0.6500  Val Acc: 0.6711\nEpoch 62/100  Train Loss: 0.5056  Train Acc: 0.7656  |  Val Loss: 0.6070  Val Acc: 0.6855\nEpoch 63/100  Train Loss: 0.5001  Train Acc: 0.7692  |  Val Loss: 0.6811  Val Acc: 0.6359\nEpoch 64/100  Train Loss: 0.5023  Train Acc: 0.7697  |  Val Loss: 0.6465  Val Acc: 0.6875\nEpoch 65/100  Train Loss: 0.4980  Train Acc: 0.7723  |  Val Loss: 0.5647  Val Acc: 0.7403\nEpoch 66/100  Train Loss: 0.4971  Train Acc: 0.7733  |  Val Loss: 0.6496  Val Acc: 0.6659\nEpoch 67/100  Train Loss: 0.4906  Train Acc: 0.7761  |  Val Loss: 0.6203  Val Acc: 0.6991\nEpoch 68/100  Train Loss: 0.4907  Train Acc: 0.7772  |  Val Loss: 0.5675  Val Acc: 0.7239\nEpoch 69/100  Train Loss: 0.4893  Train Acc: 0.7797  |  Val Loss: 0.5183  Val Acc: 0.7863\nEpoch 70/100  Train Loss: 0.4866  Train Acc: 0.7808  |  Val Loss: 0.6196  Val Acc: 0.7083\nEpoch 71/100  Train Loss: 0.4794  Train Acc: 0.7832  |  Val Loss: 0.5901  Val Acc: 0.7247\nEpoch 72/100  Train Loss: 0.4785  Train Acc: 0.7834  |  Val Loss: 0.5978  Val Acc: 0.7355\nEpoch 73/100  Train Loss: 0.4743  Train Acc: 0.7871  |  Val Loss: 0.5437  Val Acc: 0.7635\nEpoch 74/100  Train Loss: 0.4725  Train Acc: 0.7877  |  Val Loss: 0.6494  Val Acc: 0.6659\nEpoch 75/100  Train Loss: 0.4701  Train Acc: 0.7902  |  Val Loss: 0.7002  Val Acc: 0.6735\nEpoch 76/100  Train Loss: 0.4711  Train Acc: 0.7896  |  Val Loss: 0.5372  Val Acc: 0.7607\nEpoch 77/100  Train Loss: 0.4599  Train Acc: 0.7943  |  Val Loss: 0.6174  Val Acc: 0.6971\nEpoch 78/100  Train Loss: 0.4648  Train Acc: 0.7911  |  Val Loss: 0.6757  Val Acc: 0.6459\nEpoch 79/100  Train Loss: 0.4683  Train Acc: 0.7921  |  Val Loss: 0.6007  Val Acc: 0.7399\nEpoch 80/100  Train Loss: 0.4533  Train Acc: 0.7961  |  Val Loss: 0.5633  Val Acc: 0.7535\nEpoch 81/100  Train Loss: 0.4507  Train Acc: 0.7998  |  Val Loss: 0.7422  Val Acc: 0.6439\nEpoch 82/100  Train Loss: 0.4583  Train Acc: 0.7975  |  Val Loss: 0.4628  Val Acc: 0.8107\nEpoch 83/100  Train Loss: 0.4523  Train Acc: 0.7995  |  Val Loss: 0.6757  Val Acc: 0.6659\nEpoch 84/100  Train Loss: 0.4443  Train Acc: 0.8028  |  Val Loss: 0.5970  Val Acc: 0.7319\nEpoch 85/100  Train Loss: 0.4407  Train Acc: 0.8043  |  Val Loss: 0.5463  Val Acc: 0.7703\nEpoch 86/100  Train Loss: 0.4390  Train Acc: 0.8064  |  Val Loss: 0.4534  Val Acc: 0.8127\nEpoch 87/100  Train Loss: 0.4493  Train Acc: 0.8012  |  Val Loss: 0.5190  Val Acc: 0.7795\nEpoch 88/100  Train Loss: 0.4349  Train Acc: 0.8076  |  Val Loss: 0.5186  Val Acc: 0.7807\nEpoch 89/100  Train Loss: 0.4361  Train Acc: 0.8082  |  Val Loss: 0.5146  Val Acc: 0.7783\nEpoch 90/100  Train Loss: 0.4294  Train Acc: 0.8109  |  Val Loss: 0.5054  Val Acc: 0.7907\nEpoch 91/100  Train Loss: 0.4412  Train Acc: 0.8059  |  Val Loss: 0.4712  Val Acc: 0.8075\nEpoch 92/100  Train Loss: 0.4288  Train Acc: 0.8124  |  Val Loss: 0.5324  Val Acc: 0.7451\nEpoch 93/100  Train Loss: 0.4295  Train Acc: 0.8118  |  Val Loss: 0.6390  Val Acc: 0.7159\nEpoch 94/100  Train Loss: 0.4245  Train Acc: 0.8145  |  Val Loss: 0.5121  Val Acc: 0.7919\nEpoch 95/100  Train Loss: 0.4217  Train Acc: 0.8158  |  Val Loss: 0.4291  Val Acc: 0.8363\nEpoch 96/100  Train Loss: 0.4214  Train Acc: 0.8153  |  Val Loss: 0.4261  Val Acc: 0.8179\nEpoch 97/100  Train Loss: 0.4160  Train Acc: 0.8180  |  Val Loss: 0.5907  Val Acc: 0.7527\nEpoch 98/100  Train Loss: 0.4430  Train Acc: 0.8058  |  Val Loss: 0.4526  Val Acc: 0.8195\nEpoch 99/100  Train Loss: 0.4439  Train Acc: 0.8064  |  Val Loss: 0.3866  Val Acc: 0.8567\nEpoch 100/100  Train Loss: 0.4286  Train Acc: 0.8138  |  Val Loss: 0.5337  Val Acc: 0.7859\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": "### evaluate RNN models on test set\nTo complete evaluate_on_test, loop through the test data to get predictions, calculate accuracy, and print a classification report for model evaluation. This function can be used to evaluate the performance LSTM models too.\n\n**NOTE : to earn full marks for this section, you must adjust the network's hyperparameters so that each rnn models achieves at least 70% accuracy on the test data. If you achieve less than the required accuracy, consider adjusting your training loop and hyperparameters, such as the hidden state size and learning rate, to improve model performance.**",
   "metadata": {
    "id": "9tfvjr-8ECg4"
   }
  },
  {
   "cell_type": "code",
   "source": "# Evaluate on test data\ndef evaluate_on_test(model, test_loader):\n    model.eval()\n    ############################# TODO #############################\n    # TODO: Iterate over the test_loader, obtain model predictions,\n    # calculate accuracy, and generate a classification report.\n    ################################################################\n    y_true_test = []\n    y_pred_test = []\n\n    with torch.no_grad():\n        for texts, labels in test_loader:\n            texts, labels = texts.to(device), labels.to(device)\n            outputs = model(texts)\n            _, preds = torch.max(outputs, dim=1)\n\n            y_true_test.extend(labels.cpu().tolist())\n            y_pred_test.extend(preds.cpu().tolist())\n            \n    test_acc = accuracy_score(y_true_test, y_pred_test)\n    print(f\"\\nTest Accuracy: {test_acc:.4f}\\n\")\n\n    \n    print(classification_report(y_true_test, y_pred_test))",
   "metadata": {
    "id": "yZkeBWtgFbXA",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:26:56.371414Z",
     "iopub.execute_input": "2025-05-31T16:26:56.371956Z",
     "iopub.status.idle": "2025-05-31T16:26:56.376959Z",
     "shell.execute_reply.started": "2025-05-31T16:26:56.371935Z",
     "shell.execute_reply": "2025-05-31T16:26:56.376225Z"
    }
   },
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": "# Evaluate both RNN models on the test dataset\nevaluate_on_test(rnn, test_loader)\nevaluate_on_test(customRNN, test_loader)",
   "metadata": {
    "id": "JkpP3q-xGotx",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:26:57.853004Z",
     "iopub.execute_input": "2025-05-31T16:26:57.853292Z",
     "iopub.status.idle": "2025-05-31T16:26:58.074678Z",
     "shell.execute_reply.started": "2025-05-31T16:26:57.853243Z",
     "shell.execute_reply": "2025-05-31T16:26:58.073925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\nTest Accuracy: 0.8527\n\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00         0\n           1       1.00      0.85      0.92      2499\n\n    accuracy                           0.85      2499\n   macro avg       0.50      0.43      0.46      2499\nweighted avg       1.00      0.85      0.92      2499\n\n\nTest Accuracy: 0.8015\n\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00         0\n           1       1.00      0.80      0.89      2499\n\n    accuracy                           0.80      2499\n   macro avg       0.50      0.40      0.44      2499\nweighted avg       1.00      0.80      0.89      2499\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": "# LSTM section",
   "metadata": {
    "id": "VwlqjNQkAEBL"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### LSTM with nn.LSTM\nDefine an LSTM model using PyTorch's built-in nn.LSTM.",
   "metadata": {
    "id": "wwvNPJsoAFUf"
   }
  },
  {
   "cell_type": "code",
   "source": "class LSTMClassifier(nn.Module):\n    def __init__(self, output_size, hidden_size, vocab_size,\n                 device, bidirectional=False, n_layers=1,\n                 embedding_dimension=50):\n        super(LSTMClassifier, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.device = device\n\n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dimension, padding_idx = token2idx['<PAD>'])\n\n        ###################################### TODO #####################################\n        #                          COMPLETE THE FOLLOWING SECTION                       #\n        #################################################################################\n                # \"\"\"Define the LSTM layer and fully connected layers\"\"\"\n                #Your Code Here: Initialize an nn.LSTM layer and any required fully connected layers\n        \n        self.lstm = nn.LSTM(embedding_dimension, hidden_size, n_layers, batch_first=True, bidirectional=bidirectional)\n        \n        self.fc = nn.Linear(hidden_size, output_size)\n\n\n        #################################################################################\n        #                                   THE END                                     #\n        #################################################################################\n\n    def forward(self, inputs):\n        # Initialize hidden state and cell state with zeros\n        \n\n        ###################################### TODO #####################################\n        #                          COMPLETE THE FOLLOWING SECTION                       #\n        #################################################################################\n                # \"\"\"\n                # Implements the forward pass: first, embed the input tokens, then pass\n                # the embeddings through the LSTM layer to capture sequential dependencies.\n                # Finally, use fully connected layers to output class probabilities.\n                # \"\"\"\n                #Your Code Here\n\n        \n        x = self.embedding(inputs)\n\n        hidden = torch.zeros(self.n_layers, inputs.size(0), self.hidden_size).to(self.device)\n        cell_state = torch.zeros(self.n_layers, inputs.size(0), self.hidden_size).to(self.device)\n\n        \n        # h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_size).to(x.device)\n        # c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_size).to(x.device)\n        \n\n        out, _ = self.lstm(x, (hidden, cell_state))\n                # out: batch_size, seq_length, hidden_size\n\n        out = out[:, -1, :]\n        out = self.fc(out)\n    \n        \n        #################################################################################\n        #                                   THE END                                     #\n        #################################################################################\n\n        return out  # probabilities for each class in the output.",
   "metadata": {
    "id": "r2lPsPSc-ZQP",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:28:28.372698Z",
     "iopub.execute_input": "2025-05-31T16:28:28.373371Z",
     "iopub.status.idle": "2025-05-31T16:28:28.379898Z",
     "shell.execute_reply.started": "2025-05-31T16:28:28.373351Z",
     "shell.execute_reply": "2025-05-31T16:28:28.379147Z"
    }
   },
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": "### Train model\n\nIn this section, you should train model for multiple epochs on the training data and evaluate it on the validation data after each epoch, reporting the model's accuracy. Ensure that the model is set to train mode during training and switched to eval mode for evaluation on the validation data. The objective is to implement the training loop and, at the next , compute and report the final accuracy on the test data.\n\n**Note**: You are not allowed to use library-built trainer functions in this section; the training loop should be implemented manually.\n\n**Note**: To implement the training loop, you have the option to create a single train_model function that trains a model over multiple epochs, calculates training and validation accuracy, and logs the losses. Once written, this function can be reused for all RNN and LSTM models, allowing you to simply call it with different model instances for training. Reusing the function in this way will ensure that you receive credit for the training section of each subsequent model without needing to write separate loops , with just the correct function call.",
   "metadata": {
    "id": "Gh3-B63fFgra"
   }
  },
  {
   "cell_type": "code",
   "source": "output_size = 2\nhidden_size = 256\nvocab_size = len(token2idx)\n\n\nlstm = LSTMClassifier(output_size, hidden_size, vocab_size, device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(lstm.parameters(), lr=1e-3)",
   "metadata": {
    "id": "cnYI5bAAXhqg",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:42:03.286798Z",
     "iopub.execute_input": "2025-05-31T16:42:03.287303Z",
     "iopub.status.idle": "2025-05-31T16:42:03.295677Z",
     "shell.execute_reply.started": "2025-05-31T16:42:03.287280Z",
     "shell.execute_reply": "2025-05-31T16:42:03.294971Z"
    }
   },
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "source": "############################# TODO #############################\n# TODO: Implement the training loop\n################################################################\n\nn_epochs  = 40\n\nlstm = train_model(\n    lstm,\n    train_loader,\n    valid_loader,\n    criterion,\n    optimizer,\n    n_epochs,\n    device\n)\n",
   "metadata": {
    "id": "GQcqfE4xFmBi",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:42:05.249137Z",
     "iopub.execute_input": "2025-05-31T16:42:05.249830Z",
     "iopub.status.idle": "2025-05-31T16:44:43.187376Z",
     "shell.execute_reply.started": "2025-05-31T16:42:05.249807Z",
     "shell.execute_reply": "2025-05-31T16:44:43.186593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/40  Train Loss: 0.6450  Train Acc: 0.6211  |  Val Loss: 0.7778  Val Acc: 0.6319\nEpoch 2/40  Train Loss: 0.4821  Train Acc: 0.7793  |  Val Loss: 0.6492  Val Acc: 0.6715\nEpoch 3/40  Train Loss: 0.3841  Train Acc: 0.8327  |  Val Loss: 0.4433  Val Acc: 0.8043\nEpoch 4/40  Train Loss: 0.3391  Train Acc: 0.8572  |  Val Loss: 0.4380  Val Acc: 0.8279\nEpoch 5/40  Train Loss: 0.3039  Train Acc: 0.8741  |  Val Loss: 0.3775  Val Acc: 0.8235\nEpoch 6/40  Train Loss: 0.2838  Train Acc: 0.8838  |  Val Loss: 0.4619  Val Acc: 0.7915\nEpoch 7/40  Train Loss: 0.2602  Train Acc: 0.8957  |  Val Loss: 0.4760  Val Acc: 0.8111\nEpoch 8/40  Train Loss: 0.2454  Train Acc: 0.9035  |  Val Loss: 0.3534  Val Acc: 0.8391\nEpoch 9/40  Train Loss: 0.2189  Train Acc: 0.9155  |  Val Loss: 0.4281  Val Acc: 0.8175\nEpoch 10/40  Train Loss: 0.2003  Train Acc: 0.9261  |  Val Loss: 0.3494  Val Acc: 0.8659\nEpoch 11/40  Train Loss: 0.1748  Train Acc: 0.9361  |  Val Loss: 0.4644  Val Acc: 0.8199\nEpoch 12/40  Train Loss: 0.1445  Train Acc: 0.9488  |  Val Loss: 0.3506  Val Acc: 0.8395\nEpoch 13/40  Train Loss: 0.1189  Train Acc: 0.9611  |  Val Loss: 0.4819  Val Acc: 0.8307\nEpoch 14/40  Train Loss: 0.1047  Train Acc: 0.9666  |  Val Loss: 0.7174  Val Acc: 0.7971\nEpoch 15/40  Train Loss: 0.0919  Train Acc: 0.9721  |  Val Loss: 0.5992  Val Acc: 0.8315\nEpoch 16/40  Train Loss: 0.0703  Train Acc: 0.9808  |  Val Loss: 0.7346  Val Acc: 0.7987\nEpoch 17/40  Train Loss: 0.0671  Train Acc: 0.9818  |  Val Loss: 0.5103  Val Acc: 0.8559\nEpoch 18/40  Train Loss: 0.0702  Train Acc: 0.9809  |  Val Loss: 0.6977  Val Acc: 0.8275\nEpoch 19/40  Train Loss: 0.0651  Train Acc: 0.9829  |  Val Loss: 0.7249  Val Acc: 0.8147\nEpoch 20/40  Train Loss: 0.0639  Train Acc: 0.9827  |  Val Loss: 0.6535  Val Acc: 0.8351\nEpoch 21/40  Train Loss: 0.0507  Train Acc: 0.9872  |  Val Loss: 0.8113  Val Acc: 0.8143\nEpoch 22/40  Train Loss: 0.0460  Train Acc: 0.9890  |  Val Loss: 1.0300  Val Acc: 0.7603\nEpoch 23/40  Train Loss: 0.0476  Train Acc: 0.9883  |  Val Loss: 0.8221  Val Acc: 0.8063\nEpoch 24/40  Train Loss: 0.0410  Train Acc: 0.9906  |  Val Loss: 0.9915  Val Acc: 0.8167\nEpoch 25/40  Train Loss: 0.0337  Train Acc: 0.9926  |  Val Loss: 0.7807  Val Acc: 0.8291\nEpoch 26/40  Train Loss: 0.0373  Train Acc: 0.9912  |  Val Loss: 0.7996  Val Acc: 0.8527\nEpoch 27/40  Train Loss: 0.0332  Train Acc: 0.9928  |  Val Loss: 0.8942  Val Acc: 0.8263\nEpoch 28/40  Train Loss: 0.0418  Train Acc: 0.9903  |  Val Loss: 0.7757  Val Acc: 0.8351\nEpoch 29/40  Train Loss: 0.0399  Train Acc: 0.9901  |  Val Loss: 0.8875  Val Acc: 0.8359\nEpoch 30/40  Train Loss: 0.0320  Train Acc: 0.9928  |  Val Loss: 0.8812  Val Acc: 0.8279\nEpoch 31/40  Train Loss: 0.0268  Train Acc: 0.9942  |  Val Loss: 0.7812  Val Acc: 0.8611\nEpoch 32/40  Train Loss: 0.0264  Train Acc: 0.9945  |  Val Loss: 1.0738  Val Acc: 0.8191\nEpoch 33/40  Train Loss: 0.0435  Train Acc: 0.9893  |  Val Loss: 0.9617  Val Acc: 0.7863\nEpoch 34/40  Train Loss: 0.0384  Train Acc: 0.9905  |  Val Loss: 0.8352  Val Acc: 0.8263\nEpoch 35/40  Train Loss: 0.0281  Train Acc: 0.9938  |  Val Loss: 0.9125  Val Acc: 0.8235\nEpoch 36/40  Train Loss: 0.0261  Train Acc: 0.9941  |  Val Loss: 1.4056  Val Acc: 0.7999\nEpoch 37/40  Train Loss: 0.0296  Train Acc: 0.9928  |  Val Loss: 1.1700  Val Acc: 0.8199\nEpoch 38/40  Train Loss: 0.0269  Train Acc: 0.9934  |  Val Loss: 0.9189  Val Acc: 0.8311\nEpoch 39/40  Train Loss: 0.0232  Train Acc: 0.9946  |  Val Loss: 1.0944  Val Acc: 0.8259\nEpoch 40/40  Train Loss: 0.0221  Train Acc: 0.9951  |  Val Loss: 1.0314  Val Acc: 0.8047\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": "### Custom LSTM from Scratch\nImplement an LSTM from scratch by defining a LSTM cell and a model that combines these cells over the sequence.",
   "metadata": {
    "id": "MvLAiujIAOWh"
   }
  },
  {
   "cell_type": "code",
   "source": "class CustomLSTMCell(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(CustomLSTMCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        ###################################### TODO #####################################\n        #                          COMPLETE THE FOLLOWING SECTION                       #\n        #################################################################################\n                # \"\"\"Define Needed Layers \"\"\"\n        \n        self.xh = nn.Linear(input_size, hidden_size * 4)\n        self.hh = nn.Linear(hidden_size, hidden_size * 4)\n        \n        \n        std = 1.0 / np.sqrt(self.hidden_size)\n        for w in self.parameters():\n            w.data.uniform_(-std, std)\n\n        #################################################################################\n        #                                   THE END                                     #\n        #################################################################################\n\n    def forward(self, input, hidden, cell_state):\n        ###################################### TODO #####################################\n        #                          COMPLETE THE FOLLOWING SECTION                       #\n        #################################################################################\n                # \"\"\"Define Forward pass\"\"\"\n                    # Inputs:\n                    #       input: of shape (batch_size, input_size)\n                    #       hx: of shape (batch_size, hidden_size)\n                    # Outputs:\n                    #       hy: of shape (batch_size, hidden_size)\n                    #       cy: of shape (batch_size, hidden_size)\n\n\n        gates = self.xh(input) + self.hh(hidden)\n\n        # Get gates (i_t, f_t, g_t, o_t)\n        i_t, f_t, c_g, o_g = gates.chunk(4, 1)\n\n        input_gate = torch.sigmoid(i_t)\n        forget_gate = torch.sigmoid(f_t)\n        cell_gate = torch.tanh(c_g)\n        output_gate = torch.sigmoid(o_g)\n\n        \n        # New cell state\n        cell_state = forget_gate * cell_state + input_gate * cell_gate\n        # New hidden state\n        hidden = output_gate * torch.tanh(cell_state)\n\n        #################################################################################\n        #                                   THE END                                     #\n        #################################################################################\n\n        return hidden, cell_state # New hidden state , New cell state\n",
   "metadata": {
    "id": "JyccBL7zvwGZ",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:30:37.077444Z",
     "iopub.execute_input": "2025-05-31T16:30:37.077969Z",
     "iopub.status.idle": "2025-05-31T16:30:37.084236Z",
     "shell.execute_reply.started": "2025-05-31T16:30:37.077948Z",
     "shell.execute_reply": "2025-05-31T16:30:37.083583Z"
    }
   },
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": "class CustomLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n        super(CustomLSTM, self).__init__()\n        self.input_size = embedding_dim\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=token2idx['<PAD>'])\n        ###################################### TODO #####################################\n        #                          COMPLETE THE FOLLOWING SECTION                       #\n        #################################################################################\n                # \"\"\"Define Needed Layers \"\"\"\n        \n        self.output_size = output_size\n        self.rnn_cell = CustomLSTMCell(self.input_size, self.hidden_size)\n        self.fc = nn.Linear(self.hidden_size, self.output_size)\n\n        #################################################################################\n        #                                   THE END                                     #\n        #################################################################################\n\n    def forward(self, inputs):\n        # Initialize hidden state and cell state with zeros\n        hidden = torch.zeros(inputs.size(0), self.hidden_size).to(inputs.device)\n        cell_state = torch.zeros(inputs.size(0), self.hidden_size).to(inputs.device)\n\n        ###################################### TODO #####################################\n        #                          COMPLETE THE FOLLOWING SECTION                       #\n        #################################################################################\n                # \"\"\"\n                # Implements the forward pass: first, embed the input tokens, then pass\n                # the embeddings through the LSTM layer to capture sequential dependencies.\n                # Finally, use fully connected layers to output class probabilities.\n                # \"\"\"\n                #Your Code Here\n\n        x = self.embedding(inputs)\n        \n\n        for t in range(x.size(1)):\n            hidden, cell_state = self.rnn_cell(x[:, t, :], hidden, cell_state)\n\n        out = self.fc(hidden)\n            \n\n        #################################################################################\n        #                                   THE END                                     #\n        #################################################################################\n\n        return out  # probabilities for each class in the output.\n",
   "metadata": {
    "id": "88b8uj3Gv1JB",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:32:15.958448Z",
     "iopub.execute_input": "2025-05-31T16:32:15.958922Z",
     "iopub.status.idle": "2025-05-31T16:32:15.965247Z",
     "shell.execute_reply.started": "2025-05-31T16:32:15.958900Z",
     "shell.execute_reply": "2025-05-31T16:32:15.964475Z"
    }
   },
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": "### Train model\n\nIn this section, you should train model for multiple epochs on the training data and evaluate it on the validation data after each epoch, reporting the model's accuracy. Ensure that the model is set to train mode during training and switched to eval mode for evaluation on the validation data. The objective is to implement the training loop and, at the next , compute and report the final accuracy on the test data.",
   "metadata": {
    "id": "nx6vjFS-FnxW"
   }
  },
  {
   "cell_type": "code",
   "source": "vocab_size = len(token2idx)\nembedding_dim = 50\nhidden_size = 256\noutput_size = 2\n\n\n\n\ncustomLSTM = CustomLSTM(vocab_size, embedding_dim, hidden_size, output_size)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(customLSTM.parameters(), lr=5e-4)",
   "metadata": {
    "id": "aBNu0nfHXjsB",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:32:18.505307Z",
     "iopub.execute_input": "2025-05-31T16:32:18.505574Z",
     "iopub.status.idle": "2025-05-31T16:32:18.516460Z",
     "shell.execute_reply.started": "2025-05-31T16:32:18.505554Z",
     "shell.execute_reply": "2025-05-31T16:32:18.515883Z"
    }
   },
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "source": "############################# TODO #############################\n# TODO: Implement the training loop\n################################################################\n\nn_epochs  = 35\n\ncustomLSTM = train_model(\n    customLSTM,\n    train_loader,\n    valid_loader,\n    criterion,\n    optimizer,\n    n_epochs,\n    device\n)\n\n",
   "metadata": {
    "id": "uZVEkl2xFnxW",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:32:23.221504Z",
     "iopub.execute_input": "2025-05-31T16:32:23.221760Z",
     "iopub.status.idle": "2025-05-31T16:40:27.469991Z",
     "shell.execute_reply.started": "2025-05-31T16:32:23.221741Z",
     "shell.execute_reply": "2025-05-31T16:40:27.469297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/35  Train Loss: 0.6758  Train Acc: 0.5758  |  Val Loss: 0.7898  Val Acc: 0.1797\nEpoch 2/35  Train Loss: 0.5967  Train Acc: 0.6811  |  Val Loss: 0.5131  Val Acc: 0.7307\nEpoch 3/35  Train Loss: 0.4669  Train Acc: 0.7848  |  Val Loss: 0.3278  Val Acc: 0.8743\nEpoch 4/35  Train Loss: 0.3955  Train Acc: 0.8252  |  Val Loss: 0.3828  Val Acc: 0.8443\nEpoch 5/35  Train Loss: 0.3588  Train Acc: 0.8452  |  Val Loss: 0.5265  Val Acc: 0.7991\nEpoch 6/35  Train Loss: 0.3326  Train Acc: 0.8599  |  Val Loss: 0.7293  Val Acc: 0.6487\nEpoch 7/35  Train Loss: 0.3140  Train Acc: 0.8674  |  Val Loss: 0.2683  Val Acc: 0.8719\nEpoch 8/35  Train Loss: 0.2941  Train Acc: 0.8784  |  Val Loss: 0.4244  Val Acc: 0.8215\nEpoch 9/35  Train Loss: 0.2801  Train Acc: 0.8853  |  Val Loss: 0.4830  Val Acc: 0.7623\nEpoch 10/35  Train Loss: 0.2687  Train Acc: 0.8888  |  Val Loss: 0.2607  Val Acc: 0.9012\nEpoch 11/35  Train Loss: 0.2613  Train Acc: 0.8941  |  Val Loss: 0.4722  Val Acc: 0.7823\nEpoch 12/35  Train Loss: 0.2504  Train Acc: 0.9001  |  Val Loss: 0.4293  Val Acc: 0.8003\nEpoch 13/35  Train Loss: 0.2400  Train Acc: 0.9051  |  Val Loss: 0.4366  Val Acc: 0.8195\nEpoch 14/35  Train Loss: 0.2240  Train Acc: 0.9122  |  Val Loss: 0.3653  Val Acc: 0.8515\nEpoch 15/35  Train Loss: 0.2116  Train Acc: 0.9175  |  Val Loss: 0.3821  Val Acc: 0.8323\nEpoch 16/35  Train Loss: 0.2013  Train Acc: 0.9235  |  Val Loss: 0.3161  Val Acc: 0.8687\nEpoch 17/35  Train Loss: 0.1875  Train Acc: 0.9302  |  Val Loss: 0.7819  Val Acc: 0.7943\nEpoch 18/35  Train Loss: 0.1696  Train Acc: 0.9367  |  Val Loss: 0.4558  Val Acc: 0.8099\nEpoch 19/35  Train Loss: 0.1525  Train Acc: 0.9451  |  Val Loss: 0.4688  Val Acc: 0.8031\nEpoch 20/35  Train Loss: 0.1441  Train Acc: 0.9494  |  Val Loss: 0.4009  Val Acc: 0.8415\nEpoch 21/35  Train Loss: 0.1265  Train Acc: 0.9561  |  Val Loss: 0.3335  Val Acc: 0.8928\nEpoch 22/35  Train Loss: 0.1199  Train Acc: 0.9597  |  Val Loss: 0.5918  Val Acc: 0.8311\nEpoch 23/35  Train Loss: 0.1001  Train Acc: 0.9674  |  Val Loss: 0.4499  Val Acc: 0.8463\nEpoch 24/35  Train Loss: 0.0901  Train Acc: 0.9722  |  Val Loss: 0.6251  Val Acc: 0.8027\nEpoch 25/35  Train Loss: 0.0796  Train Acc: 0.9751  |  Val Loss: 0.7684  Val Acc: 0.7791\nEpoch 26/35  Train Loss: 0.0737  Train Acc: 0.9775  |  Val Loss: 0.5544  Val Acc: 0.8183\nEpoch 27/35  Train Loss: 0.0606  Train Acc: 0.9823  |  Val Loss: 0.5891  Val Acc: 0.8451\nEpoch 28/35  Train Loss: 0.0581  Train Acc: 0.9826  |  Val Loss: 0.6523  Val Acc: 0.8307\nEpoch 29/35  Train Loss: 0.0514  Train Acc: 0.9862  |  Val Loss: 0.7404  Val Acc: 0.8251\nEpoch 30/35  Train Loss: 0.0509  Train Acc: 0.9855  |  Val Loss: 0.5561  Val Acc: 0.8491\nEpoch 31/35  Train Loss: 0.0491  Train Acc: 0.9861  |  Val Loss: 0.9803  Val Acc: 0.8035\nEpoch 32/35  Train Loss: 0.0363  Train Acc: 0.9909  |  Val Loss: 0.8933  Val Acc: 0.8231\nEpoch 33/35  Train Loss: 0.0305  Train Acc: 0.9929  |  Val Loss: 0.6797  Val Acc: 0.8407\nEpoch 34/35  Train Loss: 0.0361  Train Acc: 0.9906  |  Val Loss: 1.0298  Val Acc: 0.7943\nEpoch 35/35  Train Loss: 0.0351  Train Acc: 0.9908  |  Val Loss: 0.7742  Val Acc: 0.8423\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": "### evaluate LSTM models on test set\nTo complete evaluate_on_test, loop through the test data to get predictions, calculate accuracy, and print a classification report for model evaluation.\nyou can use the `evaluate_on_test` function implemented in the previous section. Alternatively, you may write a new function to conduct this evaluation.ensure report the \"classification_report\" of both LSTM models.\n\n**NOTE : to earn full marks for this section, you must adjust the network's hyperparameters so that each lstm models achieves at least 80% accuracy on the test data. If you achieve less than the required accuracy, consider adjusting your training loop and hyperparameters, such as the hidden state size and learning rate, to improve model performance.**",
   "metadata": {
    "id": "wYv7jyi4F2j5"
   }
  },
  {
   "cell_type": "code",
   "source": "# Evaluate both LSTM models on the test dataset\nevaluate_on_test(lstm, test_loader)\nevaluate_on_test(customLSTM, test_loader)",
   "metadata": {
    "id": "K0Yyousdi7ps",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:44:50.465737Z",
     "iopub.execute_input": "2025-05-31T16:44:50.466429Z",
     "iopub.status.idle": "2025-05-31T16:44:50.822518Z",
     "shell.execute_reply.started": "2025-05-31T16:44:50.466407Z",
     "shell.execute_reply": "2025-05-31T16:44:50.821731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\nTest Accuracy: 0.8355\n\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00         0\n           1       1.00      0.84      0.91      2499\n\n    accuracy                           0.84      2499\n   macro avg       0.50      0.42      0.46      2499\nweighted avg       1.00      0.84      0.91      2499\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\nTest Accuracy: 0.8575\n\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00         0\n           1       1.00      0.86      0.92      2499\n\n    accuracy                           0.86      2499\n   macro avg       0.50      0.43      0.46      2499\nweighted avg       1.00      0.86      0.92      2499\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": "# Testing RNN and LSTM Models on a New Review",
   "metadata": {
    "id": "iML4dcBmAc95"
   }
  },
  {
   "cell_type": "code",
   "source": "# Example review\nreview = \"It is no wonder that the film has such a high rating, it is quite literally breathtaking. What can I say that hasn't said before? Not much, it's the story, the acting, the premise, but most of all, this movie is about how it makes you feel. Sometimes you watch a film, and can't remember it days later, this film loves with you, once you've seen it, you don't forget.\"\n",
   "metadata": {
    "id": "B14PDnHiCMzH",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:40:36.871892Z",
     "iopub.execute_input": "2025-05-31T16:40:36.872174Z",
     "iopub.status.idle": "2025-05-31T16:40:36.875775Z",
     "shell.execute_reply.started": "2025-05-31T16:40:36.872154Z",
     "shell.execute_reply": "2025-05-31T16:40:36.874904Z"
    }
   },
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": "## Preprocess the test Review\nTo prepare the review for the model, we need to follow similar preprocessing steps as we did for the dataset:\n\nRemove special characters and convert the text to lowercase.\nTokenize the text into individual words.\nRemove stopwords to focus only on meaningful words.\nConvert tokens to indices based on the token2idx dictionary created earlier.\nPad or truncate the sequence to a length of max_len .\n",
   "metadata": {
    "id": "3hql6MyMCK6_"
   }
  },
  {
   "cell_type": "code",
   "source": "def tokenize(text, stop_words):\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = text.lower()\n    tokens = wordpunct_tokenize(text)\n    tokens = [token for token in tokens if token not in stop_words]\n    return tokens\n\ndef remove_rare_words(tokens, common_tokens, max_len):\n    return [token if token in common_tokens\n            else '<UNK>' for token in tokens][-max_len:]\n\n# How many of the most common vocab words to keep\n# Uncommon words get replaced with unknown token <UNK>\nmax_vocab = 2500\n\n# How many tokens long each sequence will be cut to\n# Shorter sequences will get the padding token <PAD>\nmax_len = 100\n\nsequences, targets, token2idx = load_and_preprocess_data(DATA_PATH, max_vocab, max_len)\n\n\n\ndef load_and_preprocess_data(data_path, max_vocab, max_len):\n    df = pd.read_csv(data_path)\n    stop_words = set(stopwords.words('english'))\n\n    # Clean and tokenize\n    df['tokens'] = df['text'].apply(lambda x: tokenize(x, stop_words))\n\n    # Replace rare words with <UNK>\n    all_tokens = [token for tokens in df['tokens'] for token in tokens]\n    common_tokens = set(list(zip(*Counter(all_tokens).most_common(max_vocab)))[0])\n    df['tokens'] = df['tokens'].apply(lambda x: remove_rare_words(x, common_tokens, max_len))\n\n    # Remove sequences with only <UNK>\n    df = df[df['tokens'].apply(lambda tokens: any(token != '<UNK>' for token in tokens))]\n\n    # Build vocab\n    vocab = sorted(set([token for tokens in df['tokens'] for token in tokens]))\n    token2idx = {token: idx for idx, token in enumerate(vocab)}\n    token2idx['<PAD>'] = len(token2idx)\n\n    # Index tokens\n    df['indexed_tokens'] = df['tokens'].apply(lambda tokens: [token2idx[token] for token in tokens])\n\n    return df['indexed_tokens'].tolist(), df['label'].tolist(), token2idx",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:40:38.340176Z",
     "iopub.execute_input": "2025-05-31T16:40:38.340880Z",
     "iopub.status.idle": "2025-05-31T16:40:48.320632Z",
     "shell.execute_reply.started": "2025-05-31T16:40:38.340852Z",
     "shell.execute_reply": "2025-05-31T16:40:48.320052Z"
    }
   },
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport re\nfrom nltk import wordpunct_tokenize\nfrom nltk.corpus import stopwords\n\n\n# Preprocessing function\ndef preprocess_text(text, stop_words, token2idx, max_len):\n\n    ########################### TODO ###########################\n    # Step 1: Clean and lowercase the input text\n    ################################################################\n\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = text.lower()\n\n    ########################### TODO ###########################\n    # Step 2: Tokenize the text into words\n    # - Use wordpunct_tokenize to split the cleaned text into individual word tokens.\n    ############################################################\n\n    tokens = wordpunct_tokenize(text)\n\n    ########################### TODO ###########################\n    # Step 3: Remove stopwords from the token list\n    ############################################################\n\n    tokens = [token for token in tokens if token not in stop_words]\n\n    ########################### TODO ###########################\n    # Step 4: Convert tokens to indices based on the token2idx dictionary\n    # - For each token in the list, get the corresponding index from the token2idx dictionary.\n    # - If a token is not found in token2idx, replace it with the index of '<UNK>'.\n    ################################################################\n\n    unk_idx = token2idx['<UNK>']\n    tokens_idx = [token2idx.get(token, unk_idx) for token in tokens]\n\n    ########################### TODO ###########################\n    # Step 5: Pad or truncate the tokens_idx list to the desired max_len\n    ############################################################\n\n    if len(tokens_idx) >= max_len:\n        # truncate to max_len\n        tokens_idx = tokens_idx[:max_len]\n    else:\n        # pad with the <PAD> index\n        pad_idx = token2idx['<PAD>']\n        tokens_idx = tokens_idx + [pad_idx] * (max_len - len(tokens_idx))\n\n    ########################### End of TODOs ###########################\n\n    return tokens_idx  # Return the processed list of indices\n\n# Get stopwords\nstop_words = set(stopwords.words('english'))\n\n########################### TODO ###########################\n# Preprocess the review\nreview_indices = preprocess_text(review, stop_words, token2idx, max_len)\n############################################################\n\n# Convert the indices to a tensor and move it to the device (GPU or CPU)\ninput_tensor = torch.LongTensor([review_indices]).to(device)",
   "metadata": {
    "id": "vKpDw77QBC84",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:40:48.321883Z",
     "iopub.execute_input": "2025-05-31T16:40:48.322513Z",
     "iopub.status.idle": "2025-05-31T16:40:48.332865Z",
     "shell.execute_reply.started": "2025-05-31T16:40:48.322493Z",
     "shell.execute_reply": "2025-05-31T16:40:48.332192Z"
    }
   },
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": "## Make Predictions\nNow that we have preprocessed the review, use both the RNN and LSTM models to make predictions on the sentiment of the review.\n\nSet the model to evaluation mode to prevent updates during inference.\nPredict the sentiment class by passing the input_tensor to the model.\nInterpret the prediction as either \"Positive\" or \"Negative\" based on the model's output.",
   "metadata": {
    "id": "XqvvQrhjCSNY"
   }
  },
  {
   "cell_type": "code",
   "source": "def predict_sentiment(model, input_tensor, model_name=\"Model\"):\n    model.eval()  # Set the model to evaluation mode\n    ############################# TODO #############################\n    # TODO: Perform a forward pass with the model on the input_tensor,\n    # get the predicted class label, and map it to \"Positive\" or \"Negative\".\n    ################################################################\n\n    with torch.no_grad():\n        logits = model(input_tensor)                     # (1, 2) raw scores\n        probs  = torch.softmax(logits, dim=1)            # (1, 2) probabilities\n        pred_i = torch.argmax(probs, dim=1).item()       # scalar 0 or 1\n\n    class_label = \"Positive\" if pred_i == 1 else \"Negative\"\n\n    print(f\"The predicted class for the review by {model_name} is: {class_label}\")",
   "metadata": {
    "id": "4LtJcRwdCfyK",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:40:48.333567Z",
     "iopub.execute_input": "2025-05-31T16:40:48.333831Z",
     "iopub.status.idle": "2025-05-31T16:40:48.352703Z",
     "shell.execute_reply.started": "2025-05-31T16:40:48.333815Z",
     "shell.execute_reply": "2025-05-31T16:40:48.352086Z"
    }
   },
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "source": "# Make predictions using with \"predict_sentiment\" function for each of four models above\n\npredict_sentiment(rnn, input_tensor, \"RNN\")\npredict_sentiment(customRNN, input_tensor, \"CustomRNN\")\npredict_sentiment(lstm, input_tensor, \"LSTM\")\npredict_sentiment(customLSTM, input_tensor, \"CustomLSTM\")\n\n",
   "metadata": {
    "id": "fHP6-svUCp_h",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-31T16:44:57.139018Z",
     "iopub.execute_input": "2025-05-31T16:44:57.139323Z",
     "iopub.status.idle": "2025-05-31T16:44:57.177983Z",
     "shell.execute_reply.started": "2025-05-31T16:44:57.139302Z",
     "shell.execute_reply": "2025-05-31T16:44:57.177441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "The predicted class for the review by RNN is: Positive\nThe predicted class for the review by CustomRNN is: Positive\nThe predicted class for the review by LSTM is: Positive\nThe predicted class for the review by CustomLSTM is: Positive\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "source": "# Questions\n\n[1] - Based on your observations, what do you think caused the difference in performance between the RNN and LSTM models (on test set)? Analyze this difference using the results from the notebook, and discuss where a simple RNN might perform better.\nOne of the main disadvantages of RNN is vanishing gradient, which limits its ability to retain past information. In contrast, LSTM handles long-term dependencies better. It can solve this problem by its special design. \nIf the provided text is short with simple words and phrases, RNN may perform better.\n\n\n\n\n\n[2] - If we increase max_len in the preprocessing step to 300, what changes in models (rnn & lstm ) performance would you expect, and why? Please *explain* and discuss the impact this may have on the learning process and the final results.\nRNN: Each gradient term now contains up-to 300 Jacobian products, so stronger vanishing/exploding-gradient can occur. The RNN could learn farther-back dependencies, but in practice it tends to memorise local patterns and treat the rest as noise, causing over-fitting. \nLSTM: The LSTM can use information from much farther back: better sentiment understanding. but it needs more computation. \n\nI expect the LSTM’s accuracy stay the same or improve, but for RNN the test accuracy is decreased.",
   "metadata": {
    "id": "NOclaPx9EOrY"
   }
  }
 ]
}
